#!/usr/bin/env python3
"""
Scan trace files, collect failed tasks per model, and keep only the tasks that
have corresponding fixes on disk. Outputs a JSON mapping of model name to the
tasks that should be rerun with fixes applied.
"""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Set


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Generate a JSON mapping of model_name -> [failed tasks with fixes]. "
            "Traces must be JSON files generated by HAL."
        )
    )
    parser.add_argument(
        "--traces-dir",
        default="traces",
        help="Directory containing HAL trace JSON files (default: %(default)s).",
    )
    parser.add_argument(
        "--fixes-root",
        default="fixes",
        help="Root directory containing benchmark fix folders (default: %(default)s).",
    )
    parser.add_argument(
        "--output",
        help="Optional path to write the resulting JSON mapping. Prints to stdout if omitted.",
    )
    parser.add_argument(
        "--benchmark",
        help="If provided, only consider traces whose benchmark_name matches this value.",
    )
    return parser.parse_args()


def load_json(path: Path) -> Dict:
    try:
        with path.open("r", encoding="utf-8") as fh:
            return json.load(fh)
    except Exception as exc:  # pragma: no cover - defensive logging
        print(f"[WARN] Failed to parse {path}: {exc}", file=sys.stderr)
        return {}


def find_failed_tasks(
    traces_dir: Path, fixes_root: Path, benchmark_filter: str | None
) -> Dict[str, Set[str]]:
    model_to_tasks: Dict[str, Set[str]] = {}

    for trace_path in sorted(traces_dir.glob("*.json")):
        data = load_json(trace_path)
        if not data:
            continue

        config = data.get("config", {})
        agent_args = config.get("agent_args", {})
        model_name = agent_args.get("model_name") or config.get("model_name")
        benchmark_name = config.get("benchmark_name")
        if benchmark_filter and benchmark_name != benchmark_filter:
            continue

        if not model_name or not benchmark_name:
            print(
                f"[WARN] Missing model or benchmark in {trace_path}, skipping.",
                file=sys.stderr,
            )
            continue

        failed_tasks: List[str] = data.get("results", {}).get("failed_tasks", [])
        if not failed_tasks:
            continue

        benchmark_fix_dir = fixes_root / benchmark_name
        for task_id in failed_tasks:
            fix_dir = benchmark_fix_dir / task_id
            if fix_dir.is_dir():
                model_to_tasks.setdefault(model_name, set()).add(task_id)

    return model_to_tasks


def main() -> None:
    args = parse_args()
    traces_dir = Path(args.traces_dir)
    fixes_root = Path(args.fixes_root)

    if not traces_dir.is_dir():
        raise SystemExit(f"Traces directory not found: {traces_dir}")
    if not fixes_root.is_dir():
        raise SystemExit(f"Fixes root not found: {fixes_root}")

    model_to_tasks = find_failed_tasks(traces_dir, fixes_root, args.benchmark)
    serialized = {model: sorted(tasks) for model, tasks in sorted(model_to_tasks.items())}

    output_json = json.dumps(serialized, indent=2)
    if args.output:
        output_path = Path(args.output)
        output_path.write_text(output_json + "\n", encoding="utf-8")
        print(f"Wrote mapping to {output_path}")
    else:
        print(output_json)


if __name__ == "__main__":
    main()
