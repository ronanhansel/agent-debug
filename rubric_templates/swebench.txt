# SWE-bench Intrinsic Formation Error Detection Rubric

## Purpose
This rubric evaluates whether a task failure on the SWE-bench benchmark stems from an **Intrinsic Formation Error (IFE)** in the benchmark itself, rather than agent capability limitations.

## Scoring
- **Score 1**: An intrinsic benchmark defect exists AND caused the failure (no agent could succeed)
- **Score 0**: The failure is due to agent capability issues (a better agent could have succeeded)

## Two-Question Framework

### Question 1: Does an Intrinsic Formation Defect Exist?
Analyze whether the benchmark item contains any of the deficiency categories below.

### Question 2: Did the Defect Cause This Failure?
Even if a defect exists, determine whether it actually prevented success, or whether the agent failed for other reasons.

---

## Deficiency Categories for SWE-bench

### 1. Repository State Inconsistencies
The repository checkout state does not match what is required to reproduce or fix the issue:
- Missing commits that introduce necessary context
- Branch state conflicts with issue description
- Files referenced in the issue do not exist at the checkout point
- Dependencies at the checkout point are incompatible

### 2. Test Suite Defects
The test suite used for evaluation has intrinsic problems:
- Tests fail even with the gold patch applied
- Tests have race conditions or non-deterministic behavior
- Tests depend on external resources (network, specific hardware)
- Tests have incorrect assertions that reject valid fixes
- Test timeout is insufficient for legitimate solutions
- Tests only verify the specific file/function changed, missing broader impacts
- Tests check coding conventions (whitespace, newlines) rather than functionality
- Tests are "overly specific" - valid alternative implementations would fail
- Flaky tests: recursion depth errors, timing-dependent assertions
- Log parsing issues: Django "System check identified no issues" breaks result parsing
- Tests depend on httpbin.org or other external services that may return 403/timeout

### 3. Issue Description Ambiguity
The problem statement lacks necessary information for a solution:
- Reproduction steps are incomplete or incorrect
- Expected behavior is not clearly defined
- Multiple valid interpretations exist with no way to disambiguate
- Critical context is missing from the issue
- Issue references APIs, functions, or behaviors not present at checkout
- Issue assumes knowledge from linked discussions/PRs not included in prompt
- Edge cases or input constraints are not specified
- Success criteria are vague ("should work better", "fix the bug")
- Issue conflates multiple problems requiring different fixes

### 4. Environment and Dependency Issues
The execution environment prevents valid solutions:
- Required packages cannot be installed in the sandbox
- System dependencies are missing (compilers, libraries)
- Python version incompatibility with repository code
- Docker container lacks necessary tools or permissions

### 5. Evaluation Harness Bugs
The SWE-bench evaluation infrastructure has defects:
- Patch application fails for valid patches
- Conda environment setup fails
- Test collection errors unrelated to the fix
- Result parsing incorrectly marks success as failure

### 6. Gold Patch Inadequacy
The reference solution has problems:
- Gold patch does not actually fix the issue
- Gold patch introduces new bugs
- Multiple valid fixes exist but only one is accepted
- Gold patch relies on changes outside the allowed scope
- Gold patch makes assumptions not stated in the issue description
- Gold patch is one of many "divergent but valid" implementations (~47% of patches)
- Gold patch includes "supplementary semantic changes" beyond the issue scope

### 7. Solution Leakage (Not an IFE, but affects evaluation validity)
The issue description or comments contain the solution:
- Issue description includes code snippets that are essentially the fix
- Comments in the issue thread provide the solution approach
- Linked PRs or discussions reveal the exact implementation
- Note: This doesn't make the task unsolvable, but may indicate the task tests pattern-matching rather than problem-solving

---

## CRITICAL EXCLUSIONS: Agent Capability Issues (Score 0)

Do NOT classify the following as benchmark deficiencies:

### 1. Patch Formatting Errors
- Agent produces malformed unified diff
- Patch has incorrect line numbers or context
- Agent edits wrong file or creates new files incorrectly
- These are agent output format issues, not benchmark defects

### 2. Incomplete Analysis
- Agent fails to locate the relevant code
- Agent misunderstands the bug or feature request
- Agent fixes symptoms instead of root cause
- Agent's fix introduces new test failures

### 3. Tool Misuse
- Agent fails to use available tools (file search, code navigation)
- Agent makes syntax errors in bash commands
- Agent does not verify changes before submitting
- Agent runs out of context or iterations

### 4. Scope Violations
- Agent modifies files outside the allowed scope
- Agent changes test files instead of source code
- Agent makes unnecessarily broad changes

### 5. Environment Navigation Failures
- Agent fails to find files that exist
- Agent fails to install packages that are available
- Agent gives up after recoverable errors
- Agent fails to read error messages properly

---

## Evidence Requirements

For Score 1, you MUST provide:
1. **Specific defect identification**: Quote the exact issue in the benchmark
2. **Impossibility proof**: Explain why NO agent could overcome this
3. **Exclusion of alternatives**: Show that no workaround exists

For Score 0, explain:
1. What the agent did wrong
2. What a capable agent could have done instead
3. Why the benchmark allowed for success

---

## Response Format

Respond with a JSON object:

```json
{
  "score": 0 or 1,
  "deficiency_exists": true/false,
  "deficiency_caused_failure": true/false,
  "deficiency_type": "category name or 'none'",
  "existence_reasoning": "analysis of whether a benchmark defect exists",
  "causation_reasoning": "analysis of whether the defect caused failure",
  "evidence": "specific quotes from transcript supporting conclusion"
}
```

---

## Common SWE-bench Failure Patterns

### Likely Agent Issues (Score 0):
- "FileNotFoundError" when agent searched wrong directory
- "SyntaxError" in agent-generated patch
- "Test failed" because agent's fix was incorrect
- Agent submitted empty or placeholder patch
- Agent timed out while exploring codebase

### Potential Benchmark Issues (Score 1):
- Tests fail before agent makes any changes
- Repository cannot be cloned or checked out
- Required Python version not available in container
- Gold patch verification fails in harness
- Issue references code that doesn't exist at checkout

### Known Problematic Task Patterns (Investigate Carefully):

**Django Tasks:**
- Watch for "System check identified no issues (0 silenced)" in test output - this can break result parsing
- Log output appearing before "ok" marker causes false failures

**Requests Library Tasks (psf__requests-*):**
- Tests may depend on httpbin.org availability
- Intermittent 403 errors on HTTPS requests
- Known flaky: psf__requests-1963, psf__requests-2317, psf__requests-2674

**SymPy Tasks:**
- Recursion depth errors in some tests
- Pass-to-pass test inconsistencies
- Known flaky: sympy__sympy-13177, sympy__sympy-13146

**Matplotlib Tasks:**
- Graphics-dependent tests may behave differently
- Known flaky: matplotlib__matplotlib-23987

**General Patterns to Watch:**
- Tasks where the agent's fix looks reasonable but tests fail unexpectedly
- Tasks where multiple agents produce similar "wrong" answers
- Tasks where the issue description seems clear but agents consistently misinterpret it

---

## Cross-Run Analysis Guidelines

When evaluating SWE-bench IFEs, use evidence from multiple model runs:

### Using Cross-Run Evidence:

1. **"Files don't exist" claims**: If one model claims files are missing but another model successfully:
   - Located the file at a different path (e.g., `django/db/` vs `django/django/db/`)
   - Read the file content and found the target function/class
   - Produced a valid patch against the file

   Then the "missing file" is an agent navigation error, NOT an IFE.

2. **"Only 49 files found" pattern**: Agents commonly report searching only ~49 files when they:
   - Are in the wrong directory
   - Used an incorrectly scoped search tool
   - Did not properly discover the repository structure

   This is NOT evidence of incomplete repository unless ALL models fail to find expected files.

### Strong IFE Indicators (Require ALL runs to show same issue):
1. Tests fail before any agent changes in ALL runs
2. Repository checkout fails for ALL models
3. Environment/dependency issues block ALL models identically
4. Gold patch verification fails when applied manually

### Key Difference from Browser Benchmarks:
SWE-bench failures are primarily agent capability issues because:
- File paths exist but agents search wrong directories
- Patches are malformed (placeholder hunks, fake indices)
- Tool misuse (python_interpreter for file operations)
- These are NOT environment-level blocking issues like bot detection

---

## Exploratory Analysis: Discovering Hidden Benchmark Issues

Research has shown that even SWE-bench Verified contains problematic tasks. When analyzing traces, consider exploring these areas with an open mind:

### Areas to Investigate (Suggestions, Not Rules)

**1. Test Suite Quality Issues**
Consider whether the tests might be inadequate:
- Do the tests only check the specific code path in the issue, missing broader functionality?
- Could an incorrect fix pass these tests? (Research found ~31% of tasks have weak test coverage)
- Are tests checking coding conventions (whitespace, newlines) rather than actual functionality?
- Do tests depend on external services (httpbin.org, network resources) that may be unreliable?
- Look for flaky test patterns: recursion depth errors, timing issues, random failures

**2. Issue Description Problems**
Examine whether the issue statement provides sufficient information:
- Is the expected behavior clearly defined, or could multiple interpretations be valid?
- Are reproduction steps complete and accurate?
- Does the issue reference code, APIs, or behaviors that don't exist at the checkout point?
- Is there missing context that would be needed to understand the correct fix?
- Could a reasonable developer disagree about what the fix should do?

**3. Solution Leakage Concerns**
Check if the issue itself reveals the solution:
- Does the issue description contain code that is essentially the fix?
- Do issue comments provide the solution approach?
- Is this testing problem-solving or just pattern matching?

**4. Gold Patch Concerns**
Question whether the reference solution is the only valid approach:
- Could alternative implementations correctly fix the issue but fail tests?
- Does the gold patch make assumptions not stated in the issue?
- Does the gold patch change more than strictly necessary?

**5. Environment and Parsing Issues**
Look for infrastructure problems:
- Django tasks: Check for "System check identified no issues" log parsing problems
- Test output parsing: Are tests passing but marked failed due to extra output?
- Dependency issues: Are external packages or services unavailable?

### Questions to Ask Yourself

As you analyze the trace, consider:

1. **"Would a human developer find this solvable?"**
   - If the issue description is unclear even to an expert, that suggests an IFE
   - If the tests would reject a reasonable alternative fix, that suggests an IFE

2. **"What would it take to succeed here?"**
   - If success requires information not in the issue or codebase, that's an IFE
   - If success requires the tests to behave differently, that's an IFE

3. **"Is the agent's confusion reasonable?"**
   - If the agent's interpretation of the issue is defensible, the issue may be ambiguous
   - If multiple agents make similar "mistakes," the task may be underspecified

4. **"What does the test actually verify?"**
   - If tests only check one narrow case, valid fixes might fail
   - If tests check unrelated behavior, the evaluation may be flawed

### Patterns Worth Deeper Investigation

When you observe these patterns, dig deeper:

- **All models fail the same task**: Might indicate a genuine IFE rather than agent limitation
- **Agent produces a reasonable-looking fix that fails tests**: Tests might be too specific
- **Issue mentions behavior that agent cannot find in code**: Checkout state issue
- **Tests pass locally but agent is marked failed**: Harness/parsing issue
- **External URL references in tests/code**: Potential network dependency issue

### Known Problematic Patterns (from Research)

These specific patterns have been documented in academic research:

1. **Divergent but valid implementations** (~47% of patches): The gold patch may be one of many valid approaches
2. **Partial coverage tests**: Tests only verify modified files, missing side effects elsewhere
3. **Regressive gold patches** (~14%): Some gold patches fix the issue but break other functionality
4. **Convention-checking tests**: Tests verify style (trailing whitespace) not functionality

### How to Report Novel Issues

If you discover a potential IFE not covered by the standard categories:

1. Describe what you observed in the trace
2. Explain why this seems like a benchmark issue rather than agent capability
3. Suggest what would need to be true for an agent to succeed
4. Note if multiple agents exhibit similar confusion

Remember: The goal is to identify tasks where NO reasonable agent could succeed due to benchmark problems, not to excuse agent failures. But also remain open to discovering novel categories of benchmark defects that may not be obvious.
