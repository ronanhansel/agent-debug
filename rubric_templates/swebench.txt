# SWE-bench Intrinsic Formation Error Detection Rubric

## Purpose
This rubric evaluates whether a task failure on the SWE-bench benchmark stems from an **Intrinsic Formation Error (IFE)** in the benchmark itself, rather than agent capability limitations.

## Scoring
- **Score 1**: An intrinsic benchmark defect exists AND caused the failure (no agent could succeed)
- **Score 0**: The failure is due to agent capability issues (a better agent could have succeeded)

## Two-Question Framework

### Question 1: Does an Intrinsic Formation Defect Exist?
Analyze whether the benchmark item contains any of the deficiency categories below.

### Question 2: Did the Defect Cause This Failure?
Even if a defect exists, determine whether it actually prevented success, or whether the agent failed for other reasons.

---

## Deficiency Categories for SWE-bench

### 1. Repository State Inconsistencies
The repository checkout state does not match what is required to reproduce or fix the issue:
- Missing commits that introduce necessary context
- Branch state conflicts with issue description
- Files referenced in the issue do not exist at the checkout point
- Dependencies at the checkout point are incompatible

### 2. Test Suite Defects
The test suite used for evaluation has intrinsic problems:
- Tests fail even with the gold patch applied
- Tests have race conditions or non-deterministic behavior
- Tests depend on external resources (network, specific hardware)
- Tests have incorrect assertions that reject valid fixes
- Test timeout is insufficient for legitimate solutions

### 3. Issue Description Ambiguity
The problem statement lacks necessary information for a solution:
- Reproduction steps are incomplete or incorrect
- Expected behavior is not clearly defined
- Multiple valid interpretations exist with no way to disambiguate
- Critical context is missing from the issue

### 4. Environment and Dependency Issues
The execution environment prevents valid solutions:
- Required packages cannot be installed in the sandbox
- System dependencies are missing (compilers, libraries)
- Python version incompatibility with repository code
- Docker container lacks necessary tools or permissions

### 5. Evaluation Harness Bugs
The SWE-bench evaluation infrastructure has defects:
- Patch application fails for valid patches
- Conda environment setup fails
- Test collection errors unrelated to the fix
- Result parsing incorrectly marks success as failure

### 6. Gold Patch Inadequacy
The reference solution has problems:
- Gold patch does not actually fix the issue
- Gold patch introduces new bugs
- Multiple valid fixes exist but only one is accepted
- Gold patch relies on changes outside the allowed scope

---

## CRITICAL EXCLUSIONS: Agent Capability Issues (Score 0)

Do NOT classify the following as benchmark deficiencies:

### 1. Patch Formatting Errors
- Agent produces malformed unified diff
- Patch has incorrect line numbers or context
- Agent edits wrong file or creates new files incorrectly
- These are agent output format issues, not benchmark defects

### 2. Incomplete Analysis
- Agent fails to locate the relevant code
- Agent misunderstands the bug or feature request
- Agent fixes symptoms instead of root cause
- Agent's fix introduces new test failures

### 3. Tool Misuse
- Agent fails to use available tools (file search, code navigation)
- Agent makes syntax errors in bash commands
- Agent does not verify changes before submitting
- Agent runs out of context or iterations

### 4. Scope Violations
- Agent modifies files outside the allowed scope
- Agent changes test files instead of source code
- Agent makes unnecessarily broad changes

### 5. Environment Navigation Failures
- Agent fails to find files that exist
- Agent fails to install packages that are available
- Agent gives up after recoverable errors
- Agent fails to read error messages properly

---

## Evidence Requirements

For Score 1, you MUST provide:
1. **Specific defect identification**: Quote the exact issue in the benchmark
2. **Impossibility proof**: Explain why NO agent could overcome this
3. **Exclusion of alternatives**: Show that no workaround exists

For Score 0, explain:
1. What the agent did wrong
2. What a capable agent could have done instead
3. Why the benchmark allowed for success

---

## Response Format

Respond with a JSON object:

```json
{
  "score": 0 or 1,
  "deficiency_exists": true/false,
  "deficiency_caused_failure": true/false,
  "deficiency_type": "category name or 'none'",
  "existence_reasoning": "analysis of whether a benchmark defect exists",
  "causation_reasoning": "analysis of whether the defect caused failure",
  "evidence": "specific quotes from transcript supporting conclusion"
}
```

---

## Common SWE-bench Failure Patterns

### Likely Agent Issues (Score 0):
- "FileNotFoundError" when agent searched wrong directory
- "SyntaxError" in agent-generated patch
- "Test failed" because agent's fix was incorrect
- Agent submitted empty or placeholder patch
- Agent timed out while exploring codebase

### Potential Benchmark Issues (Score 1):
- Tests fail before agent makes any changes
- Repository cannot be cloned or checked out
- Required Python version not available in container
- Gold patch verification fails in harness
- Issue references code that doesn't exist at checkout
