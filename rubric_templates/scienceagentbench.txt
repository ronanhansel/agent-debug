# ScienceAgentBench Intrinsic Formation Error Detection Rubric

## Purpose
This rubric evaluates whether a task failure on the ScienceAgentBench benchmark stems from an **Intrinsic Formation Error (IFE)** in the benchmark itself, rather than agent capability limitations.

## Scoring
- **Score 1**: An intrinsic benchmark defect exists AND caused the failure (no agent could succeed)
- **Score 0**: The failure is due to agent capability issues (a better agent could have succeeded)

## Two-Question Framework

### Question 1: Does an Intrinsic Formation Defect Exist?
Analyze whether the benchmark item contains any of the deficiency categories below.

### Question 2: Did the Defect Cause This Failure?
Even if a defect exists, determine whether it actually prevented success, or whether the agent failed for other reasons.

---

## Deficiency Categories for ScienceAgentBench

### 1. Dataset and Input File Issues
The provided data files have problems:
- Data files are missing or corrupted
- File format differs from documentation
- Data contains errors that prevent valid analysis
- Dataset preview is misleading or incomplete
- Required columns or fields are missing

### 2. Task Instruction Ambiguity
The scientific task description is insufficient:
- Required methodology is not specified
- Output format is ambiguous
- Scientific domain knowledge requirements unclear
- Multiple valid interpretations exist
- Expected precision/accuracy not defined

### 3. Evaluation Script Defects
The task-specific evaluation has problems:
- Evaluation script has bugs
- Numerical tolerance is too strict
- Output comparison is format-sensitive beyond reason
- Evaluation crashes on valid outputs
- Metrics don't match task description

### 4. Gold Program Issues
The reference solution has problems:
- Gold program produces incorrect results
- Gold program uses unavailable libraries
- Gold program has hardcoded paths
- Multiple valid approaches rejected unfairly
- Gold program doesn't match task requirements

### 5. Domain Knowledge Gaps
The provided scientific context is inadequate:
- Required formulas or methods not provided
- Domain knowledge contains errors
- Scientific assumptions are unstated
- Field-specific conventions not documented
- Required background exceeds reasonable expectations

### 6. Execution Environment Issues
The Docker evaluation environment is deficient:
- Required scientific libraries unavailable
- Package versions cause compatibility issues
- Memory limits prevent large dataset processing
- GPU requirements not met
- Timeout insufficient for valid computations
- Domain-specific packages not installed (oggm, mastml, mne, biopsykit)
- Library version conflicts between dependencies
- pipreqs/pip-tools installation fails for required packages

### 8. Figure/Visualization Evaluation Issues
The GPT-4 based figure comparison is problematic:
- Color scheme differences penalized despite functional equivalence
- Axis label formatting differences cause failures
- Layout/orientation differences rejected unfairly
- Subjective quality assessment varies between runs
- Functionally equivalent visualizations scored differently
- Minor stylistic differences treated as failures

### 7. Output Specification Mismatches
The expected output format has issues:
- Output filename requirements unclear
- Data format (CSV, JSON, etc.) not specified
- Numerical precision requirements unstated
- Directory structure expectations unclear

---

## CRITICAL EXCLUSIONS: Agent Capability Issues (Score 0)

Do NOT classify the following as benchmark deficiencies:

### 1. Scientific Reasoning Errors
- Agent misunderstands the scientific problem
- Agent applies wrong methodology
- Agent makes mathematical errors
- Agent misinterprets domain knowledge
- These are agent capability issues, not benchmark defects

### 2. Code Quality Issues
- Agent writes syntactically invalid Python
- Agent has logic bugs in implementation
- Agent uses wrong libraries or APIs
- Agent fails to handle edge cases
- Agent produces runtime errors

### 3. Data Handling Errors
- Agent reads data incorrectly
- Agent misparses file formats
- Agent fails to preprocess data properly
- Agent uses wrong columns or fields
- Agent doesn't handle missing values

### 4. Output Format Errors
- Agent saves to wrong filename
- Agent uses wrong output format
- Agent doesn't include required columns
- Agent outputs data in wrong order

### 5. Incomplete Solutions
- Agent gives up before completing task
- Agent produces partial results
- Agent fails to integrate multiple steps
- Agent doesn't verify outputs

### 6. Resource Management
- Agent's approach is unnecessarily memory-intensive
- Agent doesn't optimize for available resources
- Agent fails to batch process large data
- Agent times out due to inefficient code

---

## Evidence Requirements

For Score 1, you MUST provide:
1. **Specific defect identification**: Show the exact data/script/spec issue
2. **Impossibility proof**: Demonstrate that NO valid solution can succeed
3. **Environment verification**: Confirm the issue is in benchmark, not agent

For Score 0, explain:
1. What scientific approach the agent should have used
2. What implementation errors were made
3. Why the task is solvable with correct approach

---

## Response Format

Respond with a JSON object:

```json
{
  "score": 0 or 1,
  "deficiency_exists": true/false,
  "deficiency_caused_failure": true/false,
  "deficiency_type": "category name or 'none'",
  "existence_reasoning": "analysis of whether a benchmark defect exists",
  "causation_reasoning": "analysis of whether the defect caused failure",
  "evidence": "specific quotes from transcript supporting conclusion"
}
```

---

## Common ScienceAgentBench Failure Patterns

### Likely Agent Issues (Score 0):
- Agent used wrong statistical method
- Agent failed to normalize/preprocess data
- Agent produced syntactically invalid code
- Agent saved results to wrong file location
- Agent misunderstood the scientific objective
- Agent's numerical results are incorrect due to bugs

### Potential Benchmark Issues (Score 1):
- Data file is empty or contains only headers
- Evaluation script crashes with "module not found"
- Required library (scipy, sklearn) not in container
- Gold program references nonexistent data path
- Task requires proprietary data not included
- Evaluation timeout is 10s for legitimate hour-long computation

---

## ScienceAgentBench-Specific Considerations

### Scientific Validity
ScienceAgentBench tasks are derived from real scientific workflows. Consider:
- Is the scientific methodology described correctly?
- Are the expected results scientifically plausible?
- Does the domain knowledge provided match the task?

### CodeBERT Similarity
The benchmark uses CodeBERT to measure code similarity. This is a secondary metric:
- Low similarity but correct output → agent issue (stylistic)
- High similarity but wrong output → potential benchmark issue
- Focus primarily on execution success, not code similarity

### Multi-Disciplinary Nature
Tasks span biology, chemistry, physics, etc.:
- Domain-specific library issues may be benchmark defects
- Agent's failure to apply domain knowledge is agent issue
- Missing domain context in task description could be benchmark issue

### Self-Debug Mechanism
If agent uses self-debugging:
- Failure to correct after feedback → agent issue
- Incorrect feedback from environment → potential benchmark issue
- Agent ignoring valid error messages → agent issue

---

## Cross-Run Analysis Guidelines

When evaluating ScienceAgentBench IFEs, use evidence from multiple model runs:

### Key Statistics from Trace Analysis
- **67 out of 102 tasks failed across ALL 4 models** (GPT-4.1, O3, O4-mini-high, O4-mini-low)
- **39 tasks** have runtime errors in 2+ models
- **13 tasks** have figure comparison failures in 2+ models
- Best model (O4-mini-high) achieved only 21.6% success rate

### Strong IFE Indicators:
1. **Same error across ALL models**: If all 4 models fail with similar errors (e.g., ModuleNotFoundError for 'oggm'), likely environment issue
2. **Valid program but failed evaluation**: If multiple models produce valid code (no runtime errors) but fail evaluation, investigate evaluation criteria
3. **Timeout across models**: If 2+ models timeout on same task, timeout may be insufficient
4. **Figure evaluation subjectivity**: If GPT-4 judge criticizes minor stylistic differences, evaluation may be unfairly strict

### Weak IFE Indicators (Lean toward Score 0):
1. **Only one model fails with an error**: Likely agent-specific issue
2. **Different error types across models**: Suggests agent implementation differences, not benchmark defect
3. **Some models succeed on the task**: Proves task is solvable

### Specific Patterns to Investigate:

**Module/Import Errors (Potential IFE):**
- `oggm` / `oggm.core.distribute_2d` - Glacier modeling library
- `mastml.features` - Materials science ML library
- `mne` - Neuroimaging analysis library
- `biopsykit` - Biosignal processing library
If these appear across multiple models, the environment may be missing domain-specific packages.

**Figure Comparison Failures (Potential IFE):**
Tasks where the agent produces valid output but fails due to GPT-4 judge comparison:
- Check if differences are substantive (wrong data) vs stylistic (colors, labels)
- If agent's figure shows correct scientific content but different formatting, this may be an IFE
- The benchmark documentation acknowledges "evaluation noise" in figure quality judgment

---

## Exploratory Analysis: Discovering Hidden Benchmark Issues

Research on ScienceAgentBench acknowledges several evaluation limitations. When analyzing traces, explore these areas:

### Areas to Investigate (Suggestions, Not Rules)

**1. Domain-Specific Library Availability**
Consider whether required scientific libraries are actually available:
- Does the task require specialized packages (molecular dynamics, GIS, neuroimaging)?
- Does the Docker environment have these packages installed?
- Are there version conflicts between scientific libraries?

**2. Evaluation Script Fairness**
Examine whether the evaluation is too strict:
- For figure tasks: Is the GPT-4 judge penalizing equivalent but different visualizations?
- For numerical tasks: Is the tolerance appropriate for the scientific domain?
- For data processing: Are multiple valid approaches accepted?

**3. Task Specification Completeness**
Check if the task provides sufficient information:
- Are the expected output format/structure clearly defined?
- Does the task assume knowledge from the source paper that isn't provided?
- Are success criteria (ROC-AUC thresholds, similarity scores) reasonable?

**4. Data File Issues**
Investigate whether the data files are usable:
- Are column names consistent with task description?
- Is the data format (CSV, JSON, images) clearly specified?
- Are there missing values or formatting issues in the data?

**5. Gold Program Quality**
Question whether the reference implementation is the only valid approach:
- Could alternative scientific methods produce valid results?
- Does the gold program make assumptions not in the task description?
- Are there domain-specific conventions the agent might not know?

### Questions to Ask Yourself

1. **"Would a domain expert find this specification sufficient?"**
   - If critical scientific methodology is missing, that's an IFE
   - If output format is ambiguous, that's an IFE

2. **"Is the evaluation measuring the right thing?"**
   - Figure comparison via GPT-4 is subjective and may reject valid outputs
   - Numerical tolerances may be too strict for the scientific domain

3. **"Could the environment be the problem?"**
   - Scientific computing often requires specific library versions
   - If ModuleNotFoundError appears across models, environment is likely deficient

4. **"Is this a genuine capability gap or a setup issue?"**
   - Agent can't find the right library → might be environment issue
   - Agent uses wrong scientific method → agent capability issue

### Known Benchmark Limitations (from Research Paper)

The ScienceAgentBench authors acknowledge:

1. **Evaluation noise**: Figure quality judgment shows "subjective variance in color, scale, and labeling"
2. **Partial credit challenges**: "Functionally equivalent implementations may receive lower scores"
3. **Scale limitations**: 102 tasks may not cover all edge cases
4. **Human rater bias**: "Evaluators sometimes overlook legitimate solution alternatives"

### How to Report Novel Issues

If you discover a potential IFE not covered by standard categories:

1. Describe the specific error pattern observed
2. Note how many models encountered this issue
3. Explain why the environment/evaluation seems at fault, not the agent
4. Suggest what would need to change for agents to succeed

---

## Known Problematic Task Patterns

Based on trace analysis across 4 models:

### Tasks with Environment Issues (Investigate for IFE):
- **Task 74**: Module `oggm` / `oggm.core.distribute_2d` not found (glacier modeling)
- **Task 43**: Module `mne` not found (neuroimaging)
- **Task 2**: Module `mastml.features` not found (materials science)

### Tasks with Figure Evaluation Issues (Investigate for IFE):
- Tasks 8, 25, 28, 50, 59, 68, 84, 91, 93: GPT-4 judge rejects valid visualizations due to:
  - Color scheme differences
  - Axis label formatting
  - Layout/orientation choices
  - Minor stylistic variations

### Tasks with High Failure Rate (67/102 failed across all models):
When analyzing these universally failed tasks, consider:
- Is this genuinely difficult, or is there a systematic issue?
- Do the error patterns suggest environment problems?
- Is the evaluation criteria too strict?

Remember: The goal is to identify tasks where the benchmark setup prevents success, not to excuse poor agent performance. But with 67% of tasks failing across all 4 capable models, systematic issues should be seriously considered.
